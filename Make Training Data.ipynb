{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a Latin Hypercube of training points, then call CLASS + CLPT to make the y points for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "from time import time\n",
    "from itertools import izip\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import OrderedDict\n",
    "from classy import Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compure Latin Hyper Cube, a simple but good sampling scheme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "def make_LHC(ordered_params, N):\n",
    "    \"\"\"\n",
    "    Return a vector of points in parameter space that defines a latin hypercube.\n",
    "    :param ordered_params:\n",
    "        OrderedDict that defines the ordering, name, and ranges of parameters\n",
    "        used in the trianing data. Keys are the names, value of a tuple of (lower, higher) bounds\n",
    "    :param N:\n",
    "        Number of points per dimension in the hypercube. Default is 500.\n",
    "    :return\n",
    "        A latin hyper cube sample in HOD space in a numpy array.\n",
    "    \"\"\"\n",
    "    np.random.seed(int(time()))\n",
    "\n",
    "    points = []\n",
    "    # by linspacing each parameter and shuffling, I ensure there is only one point in each row, in each dimension.\n",
    "    for plow, phigh in ordered_params.itervalues():\n",
    "        point = np.linspace(plow, phigh, num=N)\n",
    "        np.random.shuffle(point)  # makes the cube random.\n",
    "        points.append(point)\n",
    "    return np.stack(points).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the power spectra using class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "default_params = {\n",
    "        'output': 'mPk',\n",
    "        'ln10^{10}A_s': np.log( (10**10)*2.42e-9),\n",
    "        'P_k_max_h/Mpc': 100.0,\n",
    "        'n_s': 0.96,\n",
    "        'h': 0.7,\n",
    "        #'non linear': 'halofit',\n",
    "        'omega_b': 0.022,\n",
    "        'omega_cdm': 0.1122,\n",
    "        'z_pk': 0.0}\n",
    "\n",
    "def compute_pk(input_params, outputdir):\n",
    "    \"\"\"\n",
    "    Use class to compute the power spectrum as initial conditions for the sims.\n",
    "    :param input_params:\n",
    "        Updates to the default parameters for CLASS. \n",
    "    :param outputdir:\n",
    "        Outputdir to store the power specturm. It should be the same as where picola is loaded from.\n",
    "    :return:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    params = default_params.copy()\n",
    "    \n",
    "    #params.update(input_params)\n",
    "\n",
    "    cosmo = Class()\n",
    "    cosmo.set(params)\n",
    "\n",
    "    cosmo.compute()#level = [\"initnonlinear\"])\n",
    "\n",
    "    k_size = 600\n",
    "    ks = np.logspace(-3, 1.5, k_size).reshape(k_size,1,1)\n",
    "    zs = np.array([params['z_pk']])\n",
    "\n",
    "    pks =  cosmo.get_pk(ks, zs, k_size, 1, 1)[:,0,0]\n",
    "    # I save them here, but you could return as well. \n",
    "    np.savetxt(os.path.join(outputdir, 'class_pk.dat'), np.c_[ks[:,0,0], pks],\\\n",
    "               delimiter = ' ')\n",
    "\n",
    "    #return cosmo.sigma8()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore these grayed out cells. Instead, you should convert pk to matter-matter correlation. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "clpt_config = \"\"\"CORR_FUNC\n",
    "\n",
    "Init\n",
    "r_max           130\n",
    "r_min           1\n",
    "r_bin_num       80\n",
    "n_thread        2\n",
    "xi_file         {outputdir}/xi.txt\n",
    "pow_spec_file   {outputdir}/class_pk.dat\n",
    "\"\"\"\n",
    "\n",
    "def write_clpt_params(input_params, outputdir):\n",
    "    \"\"\"\n",
    "    Use class to compute the power spectrum as initial conditions for the sims.\n",
    "    :param input_params:\n",
    "        Updates to the default parameters for CLASS. \n",
    "    :param outputdir:\n",
    "        Outputdir to store the power specturm. It should be the same as where picola is loaded from.\n",
    "    :param jobname:\n",
    "        What to name the submitted jobs\n",
    "    :return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fname = os.path.join(outputdir, 'par.txt')\n",
    "\n",
    "    #params = default_params.deepcopy()\n",
    "    #params.update(input_params)\n",
    "    \n",
    "    \n",
    "    formatted_config = clpt_config.format(outputdir=outputdir)\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write(formatted_config)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def make_kils_command(jobname, max_time, outputdir, queue='short'):  # 'bulletmpi'):\n",
    "    '''\n",
    "    Return a list of strings that comprise a bash command to call trainingHelper.py on the cluster.\n",
    "    Designed to work on ki-ls's batch system\n",
    "    :param jobname:\n",
    "        Name of the job. Will also be used to make the parameter file and log file.\n",
    "    :param max_time:\n",
    "        Time for the job to run, in hours.\n",
    "    :param outputdir:\n",
    "        Directory to store output and param files.\n",
    "    :param queue:\n",
    "        Optional. Which queue to submit the job to.\n",
    "    :return:\n",
    "        Command, a list of strings that can be ' '.join'd to form a bash command.\n",
    "    '''\n",
    "    log_file = jobname + '.out'\n",
    "    param_file = jobname + '.npy'\n",
    "    command = ['bsub',\n",
    "               '-q', queue,\n",
    "               '-n', str(8),\n",
    "               '-J', jobname,\n",
    "               '-oo', os.path.join(outputdir, log_file),\n",
    "               '-W', '%d:00' % max_time,\n",
    "               '/u/ki/swmclau2/Git/CLPT_GSRSD/CLPT/clpt.exe %s'%(os.path.join(outputdir, 'par.txt'))]\n",
    "\n",
    "    return command\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordered params lists the parameters we want to sample, and the bounds we want to sample them in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#10x planck 2018 contours\n",
    "ordered_params = OrderedDict({'omega_b':[0.02233-0.00075,0.02233+0.00075 ],\n",
    "                  'omega_cdm':[0.1198-0.012/2, 0.1198+0.012/2],\n",
    "                  'ln10^{10}A_s':[3.043-0.14/2, 3.043+0.14/2],\n",
    "                  'n_s': [0.96605-0.042/2, 0.96605+0.042/2],\n",
    "                  'h': [67.37 - 5.4/2, 67.37+5.4/2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lhc = make_LHC(ordered_params, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(lhc[:,2], lhc[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_dir = './data'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "for idx, point in enumerate(lhc):\n",
    "    point_output_dirname = os.path.join(output_dir, 'point_%03d'%idx)\n",
    "    if not os.path.isdir(point_output_dirname):\n",
    "        os.mkdir(point_output_dirname)\n",
    "    \n",
    "    params = OrderedDict(izip(ordered_params.keys(), point))\n",
    "    print params\n",
    "    compute_pk(params, point_output_dirname)\n",
    "    write_clpt_params(params, point_output_dirname)\n",
    "    command = make_kils_command('albert_%03d'%idx, 1, point_output_dirname)\n",
    "    call(command, shell = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function consolidates the computed functions and puts them into one file. \n",
    "\n",
    "It is possible to write them into the output file directly in the first place, hdf5 enables that. \n",
    "\n",
    "I was just lazy when I wrote this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os import path\n",
    "import h5py\n",
    "def consolidate_outputs(directory, ordered_params, lhc,  fname= \"xi.hdf5\"):\n",
    "    \"\"\"\n",
    "    Take outputs from compute_on_subet and write them to one hdf5 file.\n",
    "    :param directory:\n",
    "        The directory with the outputs in them\n",
    "    \"\"\"\n",
    "    output_fnames = sorted(glob(path.join(directory, 'point_*/xi.txt') ) )\n",
    "\n",
    "    all_output = []\n",
    "    # i'd like to find a way to make the numpy arrays a priori but not sure how\n",
    "    n_bins = 0 \n",
    "    for o_fname in output_fnames:\n",
    "        output = np.loadtxt(o_fname)\n",
    "        all_output.append(output)\n",
    "        n_bins = output.shape[0]\n",
    "\n",
    "    all_output = np.array(all_output)\n",
    "    f = h5py.File(path.join(directory, fname), 'w')\n",
    "\n",
    "    try:\n",
    "\n",
    "        f.attrs['param_names'] = ordered_params.keys()\n",
    "        f.attrs['param_vals'] = lhc\n",
    "        print all_output.shape\n",
    "        f.attrs['scale_bins'] = all_output[0, :n_bins, 0] #\n",
    "\n",
    "\n",
    "        for point_no, data in enumerate(all_output):\n",
    "            group_name = 'point_%03d'%point_no\n",
    "            grp = f.create_group(group_name)  # could rename the above to the group name\n",
    "\n",
    "            # I could compute this, which would be faster, but this is easier to read.\n",
    "            for param_idx, param_name in enumerate(['xi_l', 'xi_0', 'xi_10', 'xi_01', 'xi_20', 'xi_11', 'xi_02']):\n",
    "                grp.create_dataset(param_name, data=data[:, param_idx+1]\\\n",
    "                                   , chunks=True, compression='gzip')\n",
    "    finally:\n",
    "        #print 'hi'\n",
    "        f.close()\n",
    "    #f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "consolidate_outputs(output_dir, ordered_params, lhc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fomr the hdf5 file get the x,y pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(filename, observable):\n",
    "    f = h5py.File(filename, 'r')\n",
    "    _x = f.attrs['param_vals']\n",
    "    r = f.attrs['scale_bins']\n",
    "    x = np.zeros((_x.shape[0]*r.shape[0], _x.shape[1]+1))\n",
    "    y = np.zeros((x.shape[0], ))\n",
    "\n",
    "    for idx, row in enumerate(_x):\n",
    "        x[idx*r.shape[0]:(idx+1)*r.shape[0], :-1] = row\n",
    "        x[idx*r.shape[0]:(idx+1)*r.shape[0], -1] = r\n",
    "        \n",
    "        key = 'point_%03d'%idx\n",
    "        grp = f[key]\n",
    "\n",
    "        y[idx*r.shape[0]:(idx+1)*r.shape[0]] = grp[observable]\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = get_data(path.join(output_dir, 'xi.hdf5')#, 'xi_l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import george\n",
    "from george.kernels import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get out an emulator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emulator_ndim = x.shape[1]\n",
    "metric = np.ones((emulator_ndim+1,))\n",
    "a = metric[0]\n",
    "kernel = a * ExpSquaredKernel(metric[1:], ndim=emulator_ndim)+\\\n",
    "            a*Matern32Kernel(metric[1:], ndim=emulator_ndim)+a\n",
    "\n",
    "emulator = george.GP(kernel)\n",
    "\n",
    "emulator.compute(x[:-8000]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = emulator.predict(y[:-8000], x[-8000:, :])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(np.abs(np.abs(y_pred - y[-8000:])/y[-8000:]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hodemulator]",
   "language": "python",
   "name": "conda-env-hodemulator-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
